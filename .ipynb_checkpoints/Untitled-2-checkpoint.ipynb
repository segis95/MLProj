{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(rotation_range=90,\n",
    "                               vertical_flip = True,horizontal_flip = True)\n",
    "\n",
    "train_gen.fit(X_train)\n",
    "print('hello')\n",
    "new_instances_X = []\n",
    "new_instances_y = []\n",
    "i = 0\n",
    "for X_batch, y_batch in train_gen.flow(X_train, Y_train, batch_size=1):\n",
    "    i+=1\n",
    "    if i == 100000:\n",
    "        break\n",
    "    new_instances_X.append(X_batch[0])\n",
    "    new_instances_y.append(y_batch[0])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = np.concatenate([X_train, np.array(new_instances_X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_aug = np.concatenate([Y_train, np.array(new_instances_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_instances_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6209 - acc: 0.4103 - val_loss: 1.4049 - val_acc: 0.4946\n",
      "Epoch 2/250\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 1.2161 - acc: 0.5704 - val_loss: 1.1506 - val_acc: 0.5879\n",
      "Epoch 3/250\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 1.0470 - acc: 0.6328 - val_loss: 1.0355 - val_acc: 0.6341\n",
      "Epoch 4/250\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 0.9206 - acc: 0.6784 - val_loss: 1.0190 - val_acc: 0.6413\n",
      "Epoch 5/250\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 0.8202 - acc: 0.7139 - val_loss: 0.9792 - val_acc: 0.6643\n",
      "Epoch 6/250\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 0.7204 - acc: 0.7477 - val_loss: 0.9299 - val_acc: 0.6875\n",
      "Epoch 7/250\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 0.6422 - acc: 0.7758 - val_loss: 0.9492 - val_acc: 0.6872\n",
      "Epoch 8/250\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 0.5621 - acc: 0.8025 - val_loss: 0.9514 - val_acc: 0.6897\n",
      "Epoch 9/250\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 0.4959 - acc: 0.8246 - val_loss: 1.0056 - val_acc: 0.6922\n",
      "10000/10000 [==============================] - 2s 193us/step\n",
      "Loss: 1.006\n",
      "Accuracy: 0.692\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "#validation_data=(X_test / 255.0, to_categorical(Y_test)),\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "\n",
    "train_gen = ImageDataGenerator()#rotation_range=90; vertical_flip = True\n",
    "train_gen.fit(X_train)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(5, 5)))# changed \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))# Added\n",
    "model.add(Conv2D(128, kernel_size=(3, 3)))# changed\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))# Added\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr = 0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(train_gen.flow(X_train, to_categorical(Y_train), batch_size=128, shuffle = True),\n",
    "          epochs=250,\n",
    "          validation_data=(X_test, to_categorical(Y_test)),\n",
    "          callbacks=[EarlyStopping(min_delta=0.001, patience=3)])\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, to_categorical(Y_test))\n",
    "\n",
    "print('Loss: %.3f' % scores[0])\n",
    "print('Accuracy: %.3f' % scores[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 16)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 12, 12, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 11, 11, 48)        6192      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               2974208   \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 84)                10836     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 3,063,606\n",
      "Trainable params: 3,063,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.Conv2D object at 0x0000025725D6D400>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000025725D6D908>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000025725D6D6A0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000025725D6D5F8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000025725D6D438>\n",
      "<keras.layers.core.Flatten object at 0x0000025725D4F9B0>\n",
      "<keras.layers.core.Dense object at 0x0000025725D45BA8>\n",
      "<keras.layers.core.Dense object at 0x0000025725D45F60>\n",
      "<keras.layers.core.Dense object at 0x00000257458914A8>\n",
      "<keras.layers.core.Dense object at 0x00000257458913C8>\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x25725d6d400>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25725d6d908>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25725d6d6a0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25725d6d5f8>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[:-6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "for layer in model.layers[:-6]:\n",
    "    layer.trainable = False\n",
    "    new_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.add(Conv2D(128, kernel_size=(3, 3)))# changed\n",
    "#new_model.add(MaxPooling2D(pool_size=(2, 2)))# Added\n",
    "new_model.add(Flatten())\n",
    "\n",
    "\n",
    "new_model.add(Dense(1000, activation='relu'))\n",
    "\n",
    "new_model.add(Dense(250, activation='relu'))\n",
    "\n",
    "new_model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 11, 11, 64)        25664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1000)              1153000   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 250)               250250    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               25100     \n",
      "=================================================================\n",
      "Total params: 1,528,318\n",
      "Trainable params: 1,502,206\n",
      "Non-trainable params: 26,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 3.8314 - acc: 0.1063 - val_loss: 3.5757 - val_acc: 0.1491\n",
      "Epoch 2/250\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.4858 - acc: 0.1598 - val_loss: 3.5720 - val_acc: 0.1559\n",
      "Epoch 3/250\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.3841 - acc: 0.1815 - val_loss: 3.4257 - val_acc: 0.1747\n",
      "Epoch 4/250\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 3.3237 - acc: 0.1901 - val_loss: 3.3427 - val_acc: 0.1875\n",
      "Epoch 5/250\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 3.2800 - acc: 0.1987 - val_loss: 3.3784 - val_acc: 0.1901\n",
      "Epoch 6/250\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 3.2443 - acc: 0.2061 - val_loss: 3.3224 - val_acc: 0.1966\n",
      "Epoch 7/250\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.1958 - acc: 0.2154 - val_loss: 3.2971 - val_acc: 0.2066\n",
      "Epoch 8/250\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.1943 - acc: 0.2152 - val_loss: 3.3006 - val_acc: 0.2019\n",
      "Epoch 9/250\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.1712 - acc: 0.2190 - val_loss: 3.3128 - val_acc: 0.1936\n",
      "Epoch 10/250\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 3.1564 - acc: 0.2234 - val_loss: 3.3169 - val_acc: 0.1949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25702d94c18>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen_100 = ImageDataGenerator()#rotation_range=90; vertical_flip = True\n",
    "train_gen_100.fit(X_train_100)\n",
    "\n",
    "# Train the model\n",
    "new_model.fit_generator(train_gen_100.flow(X_train_100, to_categorical(Y_train_100), batch_size=128, shuffle = True),\n",
    "          epochs=250,\n",
    "          validation_data=(X_test_100, to_categorical(Y_test_100)),\n",
    "          callbacks=[EarlyStopping(min_delta=0.001, patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar100\n",
    "\n",
    "(X_train_100, Y_train_100), (X_test_100, Y_test_100) = cifar100.load_data()\n",
    "\n",
    "X_train_100 = X_train_100 / 255.0\n",
    "X_test_100 = X_test_100 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(Y_test_100).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 3.6287 - acc: 0.1590 - val_loss: 3.1182 - val_acc: 0.2503\n",
      "Epoch 2/250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 2.8276 - acc: 0.3003 - val_loss: 2.8585 - val_acc: 0.3031\n",
      "Epoch 3/250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 2.3985 - acc: 0.3874 - val_loss: 2.7266 - val_acc: 0.3326\n",
      "Epoch 4/250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.9673 - acc: 0.4799 - val_loss: 2.7227 - val_acc: 0.3446\n",
      "Epoch 5/250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.4882 - acc: 0.5903 - val_loss: 2.9757 - val_acc: 0.3295\n",
      "Epoch 6/250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.0054 - acc: 0.7128 - val_loss: 3.2224 - val_acc: 0.3326\n",
      "Epoch 7/250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6110 - acc: 0.8194 - val_loss: 3.8898 - val_acc: 0.3283\n",
      "10000/10000 [==============================] - 4s 398us/step\n",
      "Loss: 3.890\n",
      "Accuracy: 0.328\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "#validation_data=(X_test / 255.0, to_categorical(Y_test)),\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "\n",
    "train_gen = ImageDataGenerator()#rotation_range=90; vertical_flip = True\n",
    "train_gen.fit(X_train)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3)))\n",
    "model.add(Conv2D(48, kernel_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(train_gen.flow(X_train, to_categorical(Y_train), batch_size=128, shuffle = True),\n",
    "          epochs=250,\n",
    "          validation_data=(X_test, to_categorical(Y_test)),\n",
    "          callbacks=[EarlyStopping(min_delta=0.001, patience=3)])\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, to_categorical(Y_test))\n",
    "\n",
    "print('Loss: %.3f' % scores[0])\n",
    "print('Accuracy: %.3f' % scores[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
